{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f838c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRAWL] Nasional — https://nasional.kompas.com/\n",
      "  [PAGE 1] https://nasional.kompas.com/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [INFO] No links found, stop.\n",
      "[CRAWL] Ekonomi — https://money.kompas.com/\n",
      "  [PAGE 1] https://money.kompas.com/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 1 unique links on page 1\n",
      "     - Fetching 1: https://video.kompas.com/watch/1873974/cara-klaim-diskon-tambah-daya-listrik-pln-50-persen-september-2025?source=KOMPASCOM&position=money_terkini__player_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [PAGE 2] https://money.kompas.com/?page=2\n",
      "   → Found 0 unique links on page 2\n",
      "  [PAGE 3] https://money.kompas.com/?page=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 3\n",
      "  [PAGE 4] https://money.kompas.com/?page=4\n",
      "   → Found 0 unique links on page 4\n",
      "  [PAGE 5] https://money.kompas.com/?page=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 5\n",
      "  [PAGE 6] https://money.kompas.com/?page=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 6\n",
      "  [PAGE 7] https://money.kompas.com/?page=7\n",
      "   → Found 0 unique links on page 7\n",
      "  [PAGE 8] https://money.kompas.com/?page=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 8\n",
      "  [PAGE 9] https://money.kompas.com/?page=9\n",
      "   → Found 0 unique links on page 9\n",
      "  [PAGE 10] https://money.kompas.com/?page=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 10\n",
      "  [PAGE 11] https://money.kompas.com/?page=11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 11\n",
      "  [PAGE 12] https://money.kompas.com/?page=12\n",
      "   → Found 0 unique links on page 12\n",
      "  [PAGE 13] https://money.kompas.com/?page=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 13\n",
      "  [PAGE 14] https://money.kompas.com/?page=14\n",
      "   → Found 0 unique links on page 14\n",
      "  [PAGE 15] https://money.kompas.com/?page=15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 15\n",
      "  [PAGE 16] https://money.kompas.com/?page=16\n",
      "   → Found 0 unique links on page 16\n",
      "  [PAGE 17] https://money.kompas.com/?page=17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 17\n",
      "  [PAGE 18] https://money.kompas.com/?page=18\n",
      "   → Found 0 unique links on page 18\n",
      "  [PAGE 19] https://money.kompas.com/?page=19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 0 unique links on page 19\n",
      "  [PAGE 20] https://money.kompas.com/?page=20\n",
      "   → Found 0 unique links on page 20\n",
      "[CRAWL] Tekno — https://tekno.kompas.com/\n",
      "  [PAGE 1] https://tekno.kompas.com/?page=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Found 29 unique links on page 1\n",
      "     - Fetching 1: https://tekno.kompas.com/read/2025/09/11/09020017/oppo-a6-gt-5g-resmi-meluncur-dengan-baterai-7.000-mah-dan-ram-12-gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 2: https://tekno.kompas.com/read/2025/09/10/14410087/ketika-layar-120-hz-baru-masuk-iphone-17-hp-android-sudah-8-tahun-lalu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 3: https://tekno.kompas.com/read/2025/09/11/08010017/foto-polaroid-gemini-ai-dipeluk-idol-k-pop-viral-ini-prompt-untuk-membuatnya\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 4: https://tekno.kompas.com/read/2025/09/09/19030087/riset-openai-ungkap-penyebab-chatbot-sering-halusinasi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 5: https://tekno.kompas.com/read/2025/09/10/11020087/pre-order-iphone-17-dibuka-12-september-di-singapura-kapan-indonesia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 6: https://tekno.kompas.com/galeri/detail/557/Unboxing.dan.Hands-on.Infinix.Hot.60.Pro.Plus.Enteng.Tipis.seperti.Tak.Bawa.HP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 7: https://tekno.kompas.com/galeri/detail/556/Membuka.Kotak.Kemasan.Oppo.Reno.14.Pro.5G.yang.Punya.Desain.Baru\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 8: https://tekno.kompas.com/galeri/detail/555/Unboxing.HP.Tipis.Samsung.Galaxy.S25.Edge.di.Taipei\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 9: http://tekno.kompas.com/read/2025/09/10/08030077/lenovo-rilis-monitor-5k-lengkung-hemat-daya-thinkvision-p40wd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 10: http://tekno.kompas.com/read/2025/09/09/16420027/lenovo-rilis-flicklift-edit-foto-di-laptop-tak-perlu-photoshop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 11: http://tekno.kompas.com/read/2025/09/09/16080017/lenovo-rilis-loq-tower-26adr10-pc-gaming-dengan-panel-transparan-dan-futuristik\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 12: https://tekno.kompas.com/read/2025/09/11/07000097/genshin-impact-6.0-dirilis-bawa-area-nod-krai-dan-karakter-baru-lauma-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 13: https://tekno.kompas.com/read/2025/09/08/11450007/saat-china-pamer-internet-10g-pertama-di-dunia-kecepatannya-hampir-10-gbps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 14: https://tekno.kompas.com/read/2025/09/10/12350017/iphone-17-rilis-harga-iphone-16-di-indonesia-turun-hingga-rp-3-jutaan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 15: https://tekno.kompas.com/read/2025/09/10/14130007/keluh-kesah-pemilik-infinix-gt-30-pro-kena-bug-reset-otomatis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 16: https://tekno.kompas.com/read/2025/09/10/19080037/oppo-a6i-meluncur-hp-murah-dengan-material-kuat-dan-baterai-besar-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 17: https://tekno.kompas.com/read/2025/09/10/18080027/hp-honor-play-10t-resmi-dengan-baterai-7.000-mah-harga-rp-2-jutaan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 18: https://tekno.kompas.com/read/2025/09/10/17350097/cara-cek-masa-aktif-telkomsel-via-online-dan-offline-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 19: https://tekno.kompas.com/read/2025/09/10/17030027/tampang-xiaomi-16-pro-max-tertangkap-kamera-punya-modul-yang-tidak-biasa-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 20: https://tekno.kompas.com/read/2025/09/10/16310037/bocoran-hp-realme-gt-8-pro-punya-modul-kamera-unik\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 21: https://tekno.kompas.com/read/2025/09/10/16030067/ulefone-rilis-ponsel-tangguh-rugone-xever-7-bisa-ganti-baterai-tanpa-matiin-hp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Fetching 22: https://tekno.kompas.com/read/2025/09/10/15430067/10-perbedaan-laptop-chromebook-dan-windows-pertimbangankan-sebelum-membeli-\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m hasil = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kategori, url \u001b[38;5;129;01min\u001b[39;00m category_urls.items():\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     hasil.extend(\u001b[43mcrawl_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkategori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_news_per_page\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# ambil lebih banyak\u001b[39;00m\n\u001b[32m    139\u001b[39m df = pd.DataFrame(hasil)\n\u001b[32m    140\u001b[39m df = df.drop_duplicates(subset=[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mcrawl_category\u001b[39m\u001b[34m(name, base_url, max_news_per_page, max_pages)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m     - Fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m news = \u001b[43mparse_news_detail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m news:\n\u001b[32m    128\u001b[39m     news_list.append(news)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mparse_news_detail\u001b[39m\u001b[34m(url, default_category)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhtml.parser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# canonical URL\u001b[39;00m\n\u001b[32m     47\u001b[39m canon_tag = soup.select_one(\u001b[33m'\u001b[39m\u001b[33mlink[rel=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcanonical\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/__init__.py:473\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.initialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/__init__.py:658\u001b[39m, in \u001b[36mBeautifulSoup._feed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.reset()\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28mself\u001b[39m.endData()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/builder/_htmlparser.py:467\u001b[39m, in \u001b[36mHTMLParserTreeBuilder.feed\u001b[39m\u001b[34m(self, markup)\u001b[39m\n\u001b[32m    464\u001b[39m parser = BeautifulSoupHTMLParser(\u001b[38;5;28mself\u001b[39m.soup, *args, **kwargs)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     parser.close()\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/html/parser.py:111\u001b[39m, in \u001b[36mHTMLParser.feed\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[32m    106\u001b[39m \n\u001b[32m    107\u001b[39m \u001b[33;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m.rawdata = \u001b[38;5;28mself\u001b[39m.rawdata + data\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/html/parser.py:171\u001b[39m, in \u001b[36mHTMLParser.goahead\u001b[39m\u001b[34m(self, end)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m, i):\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen.match(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         k = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m, i):\n\u001b[32m    173\u001b[39m         k = \u001b[38;5;28mself\u001b[39m.parse_endtag(i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/html/parser.py:338\u001b[39m, in \u001b[36mHTMLParser.parse_starttag\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_startendtag(tag, attrs)\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.CDATA_CONTENT_ELEMENTS:\n\u001b[32m    340\u001b[39m         \u001b[38;5;28mself\u001b[39m.set_cdata_mode(tag)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/builder/_htmlparser.py:182\u001b[39m, in \u001b[36mBeautifulSoupHTMLParser.handle_starttag\u001b[39m\u001b[34m(self, name, attrs, handle_empty_element)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    181\u001b[39m     sourceline = sourcepos = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m tag = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[43m=\u001b[49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[43m=\u001b[49m\u001b[43msourcepos\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag.is_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_endtag(name, check_already_closed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/__init__.py:1019\u001b[39m, in \u001b[36mBeautifulSoup.handle_starttag\u001b[39m\u001b[34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by the tree builder when a new tag is encountered.\u001b[39;00m\n\u001b[32m    998\u001b[39m \n\u001b[32m    999\u001b[39m \u001b[33;03m:param name: Name of the tag.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m \u001b[33;03m:meta private:\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;66;03m# print(\"Start tag %s: %s\" % (name, attrs))\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28mself\u001b[39m.parse_only\n\u001b[32m   1023\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.tagStack) <= \u001b[32m1\u001b[39m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_only.allow_tag_creation(nsprefix, name, attrs)\n\u001b[32m   1025\u001b[39m ):\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/__init__.py:864\u001b[39m, in \u001b[36mBeautifulSoup.endData\u001b[39m\u001b[34m(self, containerClass)\u001b[39m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    863\u001b[39m containerClass = \u001b[38;5;28mself\u001b[39m.string_container(containerClass)\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m o = \u001b[43mcontainerClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m.object_was_parsed(o)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/bs4/element.py:1300\u001b[39m, in \u001b[36mNavigableString.__new__\u001b[39m\u001b[34m(cls, value)\u001b[39m\n\u001b[32m   1292\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a new NavigableString.\u001b[39;00m\n\u001b[32m   1293\u001b[39m \n\u001b[32m   1294\u001b[39m \u001b[33;03mWhen unpickling a NavigableString, this method is called with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1297\u001b[39m \u001b[33;03mhow to handle non-ASCII characters.\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1300\u001b[39m     u = \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1302\u001b[39m     u = \u001b[38;5;28mstr\u001b[39m.\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value, DEFAULT_OUTPUT_ENCODING)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests, uuid, time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# ✅ Tambah kategori supaya data lebih banyak\n",
    "category_urls = {\n",
    "    \"Nasional\": \"https://nasional.kompas.com/\",\n",
    "    \"Ekonomi\": \"https://money.kompas.com/\",\n",
    "    \"Tekno\": \"https://tekno.kompas.com/\",\n",
    "    \"Otomotif\": \"https://otomotif.kompas.com/\",\n",
    "    \"Health\": \"https://health.kompas.com/\",\n",
    "    \"Edukasi\": \"https://edukasi.kompas.com/\",\n",
    "    \"Bola\": \"https://bola.kompas.com/\",\n",
    "    \"Entertainment\": \"https://entertainment.kompas.com/\",\n",
    "    \"Lifestyle\": \"https://lifestyle.kompas.com/\",\n",
    "    \"Travel\": \"https://travel.kompas.com/\",\n",
    "    \"Internasional\": \"https://internasional.kompas.com/\",\n",
    "    \"Properti\": \"https://properti.kompas.com/\",\n",
    "    \"Sains\": \"https://sains.kompas.com/\"\n",
    "}\n",
    "\n",
    "def canonicalize(url):\n",
    "    p = urlparse(url)\n",
    "    return urlunparse((p.scheme or \"https\", p.netloc, p.path.rstrip('/'), \"\", \"\", \"\"))\n",
    "\n",
    "def first_text(soup, selectors):\n",
    "    for sel in selectors:\n",
    "        nodes = soup.select(sel)\n",
    "        if nodes:\n",
    "            return \" \".join([n.get_text(strip=True) for n in nodes])\n",
    "    return None\n",
    "\n",
    "seen_urls = set()\n",
    "\n",
    "def parse_news_detail(url, default_category):\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"[ERROR] status {res.status_code} for {url}\")\n",
    "            return None\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # canonical URL\n",
    "        canon_tag = soup.select_one('link[rel=\"canonical\"]')\n",
    "        final_url = canon_tag['href'] if canon_tag and canon_tag.get('href') else res.url\n",
    "        url_key = canonicalize(final_url)\n",
    "        if url_key in seen_urls:\n",
    "            return None\n",
    "        seen_urls.add(url_key)\n",
    "\n",
    "        # judul\n",
    "        title = first_text(soup, [\"h1.read__title\", \"h1.article__title\", \"h1.title\"])\n",
    "        if not title:\n",
    "            m = soup.find('meta', property='og:title') or soup.find('meta', attrs={'name': 'title'})\n",
    "            title = m['content'] if m and m.get('content') else \"\"\n",
    "\n",
    "        # isi\n",
    "        content = first_text(soup, [\n",
    "            \"div.read__content p\",\n",
    "            \"div.article__content p\",\n",
    "            \"div.detail_text p\",\n",
    "            \"div.article__lead p\",\n",
    "            \"div._article_content p\"\n",
    "        ])\n",
    "        if not content:\n",
    "            m = soup.find('meta', property='og:description') or soup.find('meta', attrs={'name': 'description'})\n",
    "            content = m['content'] if m and m.get('content') else \"\"\n",
    "\n",
    "        # kategori (pakai kategori input biar konsisten)\n",
    "        breadcrumb = [a.get_text(strip=True) for a in soup.select(\"a.breadcrumb__link, span.breadcrumb__link\")]\n",
    "        detected_kategori = breadcrumb[-1] if breadcrumb else default_category\n",
    "\n",
    "        return {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"judul\": title,\n",
    "            \"isi\": content,\n",
    "            \"kategori\": default_category,           # konsisten\n",
    "            \"detected_kategori\": detected_kategori, # info tambahan\n",
    "            \"url\": final_url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR parsing] {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_category(name, base_url, max_news_per_page=None, max_pages=20):\n",
    "    print(f\"[CRAWL] {name} — {base_url}\")\n",
    "    news_list = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        if max_pages and page > max_pages:\n",
    "            break\n",
    "        page_url = f\"{base_url}?page={page}\"\n",
    "        print(f\"  [PAGE {page}] {page_url}\")\n",
    "        res = requests.get(page_url, headers=headers, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"   [ERROR] Can't access {page_url}\")\n",
    "            break\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # ambil semua link berita\n",
    "        anchors = soup.select(\"h3.article__title a, h4.article__title a, article a\")\n",
    "        if not anchors:\n",
    "            print(\"   [INFO] No links found, stop.\")\n",
    "            break\n",
    "\n",
    "        # filter & canonicalize\n",
    "        hrefs = []\n",
    "        for a in anchors:\n",
    "            href = a.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            full = urljoin(base_url, href)\n",
    "            key = canonicalize(full)\n",
    "            if key not in seen_urls and key not in hrefs:\n",
    "                hrefs.append(full)\n",
    "\n",
    "        print(f\"   → Found {len(hrefs)} unique links on page {page}\")\n",
    "\n",
    "        for idx, link in enumerate(hrefs):\n",
    "            if max_news_per_page and idx >= max_news_per_page:\n",
    "                break\n",
    "            print(f\"     - Fetching {idx+1}: {link}\")\n",
    "            news = parse_news_detail(link, name)\n",
    "            if news:\n",
    "                news_list.append(news)\n",
    "            time.sleep(1)\n",
    "\n",
    "        page += 1\n",
    "    return news_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hasil = []\n",
    "    for kategori, url in category_urls.items():\n",
    "        hasil.extend(crawl_category(kategori, url, max_news_per_page=None, max_pages=20))  # ambil lebih banyak\n",
    "\n",
    "    df = pd.DataFrame(hasil)\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "    df.to_csv(\"kompas_news_big.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[DONE] Total {len(df)} berita disimpan ke kompas_news_big.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}